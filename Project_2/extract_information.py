from pymongo import MongoClient
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import MongoDBAtlasVectorSearch
from langchain.document_loaders import DirectoryLoader
from langchain. llms import OpenAI
from langchain.chains import RetrievalQA
import gradio as gr
from gradio.themes.base import Base
from dotenv import load_dotenv
import os
import load_data
import requests

load_dotenv()


client = MongoClient(os.environ["MONGO_URI"])
dbName = "langchain_demo"
collectionName = "collection_of_text_blobs"
collection = client[dbName][collectionName]


def generate_retriever_query(query, context):
        return f"Query of user: '{query}' and context from existing documents found (dont tell anything unless user has specified): '{context}'"


def fetch_llm_output(query: str):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": "phi3",
        "prompt": query,
        "stream": False
    }

    # Set the headers (optional, but recommended for JSON payloads)
    headers = {
        "Content-Type": "application/json"
    }

    # Send the POST request
    response = requests.post(url, json=payload, headers=headers)
    if response.status_code == 200:
        return response.json()["response"]
    else:
        return f"Error: {response.status_code} - {response.text}"


def query_data(query):
   results = list(collection.aggregate([
    {
        "$vectorSearch": {
            "queryVector": load_data.generate_embedding(query),
            "path": "embedding",
            "numCandidates": 20,
            "limit": 1,
            "index": "default"
        }
    }
]))
   
#    print(results)
#    as_output = ""
#    llm_output = ""
   
   if len(results) == 0:
    print("No relavent couments found")
    retriever_query = query
    llm_output = fetch_llm_output(retriever_query)

   else:
    document = results[0]
    as_output = document['page_content']
    retriever_query = generate_retriever_query(query, as_output)
    llm_output = fetch_llm_output(retriever_query)

   return as_output, llm_output




with gr.Blocks (theme=Base(), title="Question Answering App using Vector Search + RAG") as demo:
    gr.Markdown (
    """
    # Question Answering App using Atlas Vector Search + RAG Architecture
    """)
    textbox = gr.Textbox (label="Enter your Question: ")
    with gr.Row():
        button = gr.Button("Submit", variant="primary")
    with gr.Column ( ) :
        output1 = gr.Textbox(lines=1, max_lines=10, label="Output with just Atlas Vector Search (returns)")
        output2 = gr.Textbox (lines=1, max_lines=10, label="Output generated by chaining Atlas Vector Search to Langchain Retrival QA and an LLM")
    button.click(query_data, textbox, outputs=[output1, output2])
    
    
demo.launch()




